## Asistente de Consulta TÃ©cnica

Este proyecto implementa una aplicaciÃ³n web interactiva que actÃºa como asistente de consulta tÃ©cnica. Utiliza Streamlit para la interfaz, Pinecone como almacÃ©n vectorial para bÃºsqueda semÃ¡ntica y OpenAI vÃ­a LangChain para el procesamiento de lenguaje natural.

---

## ğŸ“ Estructura de carpetas

```
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ data
â”‚ â””â”€â”€ documentos
â”‚ â”œâ”€â”€ documento1.txt
â”‚ â”œâ”€â”€ documento2.pdf
â”‚ â””â”€â”€ ...
â””â”€â”€ src
| â”œâ”€â”€ loader.py
| â”œâ”€â”€ pinecone_utils.py
  â””â”€â”€ chain.py

```
---

## ğŸ”§ MÃ³dulos principales

### 1. loader.py

- **`load_and_split_documents(path, chunk_size, chunk_overlap)`**  
   1. Recorre los archivos en `path`.  
   2. Carga cada archivo con `TextLoader`.  
   3. Los fragmenta en trozos de `chunk_size` caracteres con `chunk_overlap`.  
   4. Devuelve lista de objetos `Document` listos para indexar.

### 2. pinecone_utils.py

- **`get_pinecone_client(api_key=None)`**  
  Inicializa `Pinecone(api_key=â€¦)` usando la clave de entorno o pasada como parÃ¡metro.

- **`get_or_create_index(client, name, dimension, cloud, region)`**  
  Verifica si existe el Ã­ndice `name`; si no, lo crea en modo serverless con las specs dadas. Devuelve la instancia `client.Index(name)`.

### 3. chain.py

- **`build_vectorstore(docs)`**  
   1. Crea un objeto `OpenAIEmbeddings`.  
   2. Llama a `PineconeVectorStore.from_texts(...)` para subir los chunks al Ã­ndice.  
   3. Devuelve el vectorstore listo para recuperaciÃ³n.

- **`get_qa_chain(vectorstore)`**  
   1. Instancia `ChatOpenAI` con tu clave.  
   2. Usa `RetrievalQA.from_chain_type(...)` para enlazar el LLM con el vectorstore.  
   3. Configura `k=5` resultados y retorno de documentos fuente.

---
## âš™ï¸ ConfiguraciÃ³n y ejecuciÃ³n

## 1 Clonar el repositorio

`https://github.com/Ihan-Marroquin/proyecto_3.git`

## 2. ConfiguraciÃ³n del entorno virtual

2.1. Crea el entorno virtual en la carpeta `venv/`:
   - `python -m venv venv`

2.2. Activalo:
   - Windows:
       - `venv\Scripts\activate`
    
  - Linux/Mac:
       - `source venv/bin/activate`

Tras esto, tu prompt mostrarÃ¡ `(venv)` al inicio.

## 4. InstalaciÃ³n de dependencias
Con el entorno activado, instala todo lo necesario:

```
pip install --upgrade pip
pip install -r requirements.txt
```

El archivo `requirements.txt` incluye:

```
streamlit
openai
langchain
pinecone
python-dotenv
langchain-community
langchain-pinecone
tiktoken
```

## 5. Configurar variables de entorno
Duplica `.env.example` como `.env` y rellena tus claves:

```
OPENAI_API_KEY=tu_openai_key
PINECONE_API_KEY=tu_pinecone_key
PINECONE_ENVIRONMENT=<extraÃ­do de tu host>
PINECONE_INDEX_NAME=asistente-tecnico
PINECONE_DIMENSION=1536   # o 3072 segÃºn modelo de embeddings
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1
```

## 6 Colocar documentos
Pon tus `.txt` en `data/documentos/`. MÃ­nimo 70 archivos, la aplicacion ya cuenta con 70 archivos de muestra

## 7. Ejecutar la aplicaciÃ³n

`streamlit run app.py`

---

## ğŸ¥¸ Aprendizaje

AprendÃ­ a ver la IA como un proceso completo: desde construir una pÃ¡gina simple donde el usuario hace preguntas, hasta organizar mis documentos para que el sistema â€œentiendaâ€ y busque la informaciÃ³n mÃ¡s relevante, asi mismo, descubrÃ­ que dividir textos largos en partes mÃ¡s pequeÃ±as ayuda al modelo a procesarlos sin perder contexto, y que un buen Ã­ndice (como el de Pinecone) acelera la bÃºsqueda de esas partes, tambiÃ©n enfrentÃ© pequeÃ±os tropiezos, como errores al leer archivos o cambiar funciones de librerÃ­as, pero eso me enseÃ±Ã³ a buscar soluciones en la documentaciÃ³n y a estructurar mi cÃ³digo en mÃ³dulos claros, ademÃ¡s, este proyecto me mostrÃ³ cÃ³mo combinar varias herramientas y prÃ¡cticas de programaciÃ³n para crear un asistente de IA prÃ¡ctico y fiable.
